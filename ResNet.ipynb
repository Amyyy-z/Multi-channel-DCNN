{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\anaconda3\\envs\\New\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#Build models using Keras\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers.core import Permute\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.enable_eager_execution() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:localhost/replica:0/task:0/device:CPU:0 /job:localhost/replica:0/task:0/device:CPU:0\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 /job:localhost/replica:0/task:0/device:GPU:0\n",
      "warmup: 1.359598 0.1883961000000003\n",
      "run time: 1.3753935999999998 0.0006561000000004924\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "\tcpu_a = tf.random.normal([10000, 1000])\n",
    "\tcpu_b = tf.random.normal([1000, 2000])\n",
    "\tprint(cpu_a.device, cpu_b.device)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\tgpu_a = tf.random.normal([10000, 1000])\n",
    "\tgpu_b = tf.random.normal([1000, 2000])\n",
    "\tprint(gpu_a.device, gpu_b.device)\n",
    "\n",
    "def cpu_run():\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tc = tf.matmul(cpu_a, cpu_b)\n",
    "\treturn c\n",
    "\n",
    "def gpu_run():\n",
    "\twith tf.device('/gpu:0'):\n",
    "\t\tc = tf.matmul(gpu_a, gpu_b)\n",
    "\treturn c\n",
    "\n",
    "\n",
    "# warm up\n",
    "cpu_time = timeit.timeit(cpu_run, number=10)\n",
    "gpu_time = timeit.timeit(gpu_run, number=10)\n",
    "print('warmup:', cpu_time, gpu_time)\n",
    "\n",
    "\n",
    "cpu_time = timeit.timeit(cpu_run, number=10)\n",
    "gpu_time = timeit.timeit(gpu_run, number=10)\n",
    "print('run time:', cpu_time, gpu_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "\n",
    "#import all the images in the file Left & Right\n",
    "image_list = []\n",
    "files = glob.glob (r\"C:\\...\\Left\\*.png\")\n",
    "\n",
    "for filename in files:\n",
    "    print(filename)\n",
    "    image = cv2.imread(filename)\n",
    "    #image = tf.image.resize(image, [125, 125]) #resize images into required height and width\n",
    "    image_list.append(image) #convert images into array\n",
    "    #print(filename)\n",
    "print('image_list shape:', np.array(image_list).shape)  #check whether the right amount of images were imported\n",
    "\n",
    "\n",
    "#convert image list into array\n",
    "image_list = np.asarray(image_list)\n",
    "\n",
    "\n",
    "image_list\n",
    "print(type(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "X = image_list.reshape(1224,3,224,224).transpose(0,2,3,1).astype(\"uint8\") #reshape images, 1224 is the number of input images\n",
    "# 224*224*3 pixels\n",
    "\n",
    "plt.imshow(image_list[1018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels\n",
    "df = pd.read_csv(\"Left.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract type from table\n",
    "Label = df[\"Type\"]\n",
    "\n",
    "#Stack into array\n",
    "label = np.asarray(Label)\n",
    "label\n",
    "\n",
    "\n",
    "def one_hot_encode(vec, vals = 2):\n",
    "    #to one-hot encode the 4- possible labesl\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out\n",
    "\n",
    "#set up images and labels\n",
    "class CifarHelper():\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        \n",
    "        #self.all_train_batches = [X]\n",
    "        \n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def set_up_images(self):\n",
    "        print(\"Setting up images and labels\")\n",
    "        self.images = np.vstack([X])\n",
    "        all_len = len(self.images)\n",
    "        \n",
    "        self.images = self.images.reshape(all_len, 3, 224, 224).transpose(0,2,3,1)/255\n",
    "        self.labels = one_hot_encode(np.hstack([label]), 2)\n",
    "\n",
    "#before tensorflow run:\n",
    "ch = CifarHelper()\n",
    "ch.set_up_images()\n",
    "\n",
    "#Check the image and its label\n",
    "index = 1090\n",
    "plt.imshow(X[index])\n",
    "print(label[index])\n",
    "\n",
    "\n",
    "#Encoding data\n",
    "def vectorize_sequences(sequences, dimension = 1000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "#Encode labels\n",
    "def to_one_hot(labels, dimension=2):  #number can be updated depend on the output class\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_labels = to_one_hot(label)\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "#k-fold stratified CV for maintaining the imbalanced dataset ratio for both training and testing sets\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "kf.get_n_splits(X, label)\n",
    "\n",
    "\n",
    "#10-fold stratified CV split\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, Model, Sequential, regularizers\n",
    "\n",
    "#Model buid\n",
    "class BasicBlock(layers.Layer):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, out_channel, strides=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(out_channel, kernel_size=3, strides=strides,\n",
    "                                   padding=\"SAME\", use_bias=False,kernel_regularizer=regularizers.l2(0.001)\n",
    ")\n",
    "        self.bn1 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = layers.Conv2D(out_channel, kernel_size=3, strides=1,\n",
    "                                   padding=\"SAME\", use_bias=False,kernel_regularizer=regularizers.l2(0.001)\n",
    ")\n",
    "        self.bn2 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        # -----------------------------------------\n",
    "        self.downsample = downsample\n",
    "        self.relu = layers.ReLU()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        identity = inputs\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        x = self.add([identity, x])\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck(layers.Layer):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, out_channel, strides=1, downsample=None, **kwargs):\n",
    "        super(Bottleneck, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(out_channel, kernel_size=1, use_bias=False, name=\"conv1\",kernel_regularizer=regularizers.l2(0.001)\n",
    ")\n",
    "        self.bn1 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv1/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = layers.Conv2D(out_channel, kernel_size=3, use_bias=False,\n",
    "                                   strides=strides, padding=\"SAME\", name=\"conv2\",kernel_regularizer=regularizers.l2(0.001)\n",
    ")\n",
    "        self.bn2 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv2/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = layers.Conv2D(out_channel * self.expansion, kernel_size=1, use_bias=False, name=\"conv3\",kernel_regularizer=regularizers.l2(0.001)\n",
    ")\n",
    "        self.bn3 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv3/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.relu = layers.ReLU()\n",
    "        self.downsample = downsample\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        identity = inputs\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        x = self.add([x, identity])\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _make_layer(block, in_channel, channel, block_num, name, strides=1):\n",
    "    downsample = None\n",
    "    if strides != 1 or in_channel != channel * block.expansion:\n",
    "        downsample = Sequential([\n",
    "            layers.Conv2D(channel * block.expansion, kernel_size=1, strides=strides,\n",
    "                          use_bias=False, name=\"conv1\",kernel_regularizer=regularizers.l2(0.001)\n",
    "),\n",
    "            layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5, name=\"BatchNorm\")\n",
    "        ], name=\"shortcut\")\n",
    "\n",
    "    layers_list = []\n",
    "    layers_list.append(block(channel, downsample=downsample, strides=strides, name=\"unit_1\"))\n",
    "\n",
    "    for index in range(1, block_num):\n",
    "        layers_list.append(block(channel, name=\"unit_\" + str(index + 1)))\n",
    "\n",
    "    return Sequential(layers_list, name=name)\n",
    "\n",
    "\n",
    "def _resnet(block, blocks_num, im_width=224, im_height=224, num_classes=2, include_top=True):\n",
    "\n",
    "    # (None, 224, 224, 3)\n",
    "    input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")\n",
    "    x = layers.Conv2D(filters=64, kernel_size=7, strides=2,\n",
    "                      padding=\"SAME\", use_bias=False, name=\"conv1\",kernel_regularizer=regularizers.l2(0.001)\n",
    ")(input_image)\n",
    "    \n",
    "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv1/BatchNorm\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\")(x)\n",
    "\n",
    "    x = _make_layer(block, x.shape[-1], 64, blocks_num[0], name=\"block1\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 128, blocks_num[1], strides=2, name=\"block2\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 256, blocks_num[2], strides=2, name=\"block3\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 512, blocks_num[3], strides=2, name=\"block4\")(x)\n",
    "    \n",
    "    x = Dropout(0.5)(x)  #dropout layer can be updated depends on whether overfitting is shown\n",
    "   \n",
    "    if include_top:\n",
    "        x = layers.GlobalAvgPool2D()(x)  # pool + flatten\n",
    "        x = layers.Dense(num_classes, name=\"logits\")(x)\n",
    "        predict = layers.Softmax()(x)\n",
    "    else:\n",
    "        predict = x\n",
    "\n",
    "    model = Model(inputs=input_image, outputs=predict)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet6(im_width=224, im_height=224, num_classes=1000):\n",
    "    return _resnet(BasicBlock, [1,1,1,1], im_width, im_height, num_classes)\n",
    "\n",
    "def resnet18(im_width=224, im_height=224, num_classes=1000):\n",
    "    return _resnet(BasicBlock, [2,2,2,2], im_width, im_height, num_classes)\n",
    "\n",
    "def resnet34(im_width=224, im_height=224, num_classes=1000):\n",
    "    return _resnet(BasicBlock, [3, 4, 6, 3], im_width, im_height, num_classes)\n",
    "\n",
    "def resnet50(im_width=224, im_height=224, num_classes=1000, include_top=True):\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], im_width, im_height, num_classes, include_top)\n",
    "\n",
    "def resnet101(im_width=224, im_height=224, num_classes=1000, include_top=True):\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], im_width, im_height, num_classes, include_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_from_path_label(image, label):\n",
    "    image = 2*tf.cast(image, dtype=tf.float32) / 255.-1\n",
    "    label = tf.cast(label, dtype=tf.int32)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "import time\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "#Train and test model through CV\n",
    "CV_summary = []\n",
    "CM_summary_final = np.mat(np.zeros((2,2))) #output a 2*2 matrix for confusion matrix\n",
    "t_CV = time.perf_counter()\n",
    "\n",
    "fold = 0\n",
    "\n",
    "for i in kf.split(X, y):\n",
    "    fold += 1\n",
    "    train_image = X[i[0]]\n",
    "    train_label = one_hot_labels[i[0]]\n",
    "    \n",
    "    test_image = X[i[1]]\n",
    "    test_label = one_hot_labels[i[1]]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_image,train_label))\n",
    "    train_db = train_dataset.shuffle(1000).map(load_and_preprocess_from_path_label).batch(15) #set up batch size 15\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_image,test_label))\n",
    "    test_db = test_dataset.shuffle(1000).map(load_and_preprocess_from_path_label).batch(15)\n",
    "    \n",
    "    #print(train_db, test_db)\n",
    "    \n",
    "    #print(train_image[1])\n",
    "    \n",
    "    t_fold = time.perf_counter()\n",
    "\n",
    "    model = resnet6(num_classes = 2)\n",
    "    model.summary()\n",
    "    \n",
    "    optimizer = optimizers.Adam(lr=1e-5)  #select optimizer and learning rate\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #CV_log_dir = 'logs/CV' + TOT + str(fold) + '%%' + current_time\n",
    "    #CV_summary_writer = tf.summary.create_file_writer(CV_log_dir)\n",
    "\n",
    "    CM_summary = np.mat(np.zeros((2,2)))\n",
    "    Epoch_summary = []\n",
    "\n",
    "    epochs = 30   #number of epochs can be updated based on the model's convergence\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        train_loss.reset_states()  # clear history info\n",
    "        train_accuracy.reset_states()  # clear history info\n",
    "        test_loss.reset_states()  # clear history info\n",
    "        test_accuracy.reset_states()  # clear history info\n",
    "        summary = []\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        for step, (x,y) in enumerate(train_db):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                logits = model(x, training=True)\n",
    "                loss = tf.losses.categorical_crossentropy(y, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                train_loss(loss)\n",
    "                train_accuracy(y, logits)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print('-----------------------------------------------------------------')\n",
    "        print('Training time: ',time.perf_counter() - t1)\n",
    "\n",
    "        test_pred = []\n",
    "        test_GT = []\n",
    "        \n",
    "        t2 = time.perf_counter()\n",
    "        for xt,yt in test_db:\n",
    "\n",
    "            logits = model(xt, training=False)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            new_label = tf.argmax(yt,axis=1)\n",
    "            test_pred.extend(pred)\n",
    "            test_GT.extend(new_label)\n",
    "            t_loss = tf.losses.categorical_crossentropy(yt, logits, from_logits=True)\n",
    "            test_loss(t_loss)\n",
    "            test_accuracy(yt, logits)\n",
    " \n",
    "        CM = tf.math.confusion_matrix(test_GT,test_pred)\n",
    "        TP = CM[1,1]\n",
    "        TN = CM[0,0]\n",
    "        FP = CM[0,1]\n",
    "        FN = CM[1,0]\n",
    "\n",
    "        Acc = ((TP + TN) / (TP + TN + FP + FN))\n",
    "        PPV = TP / (TP + FP)\n",
    "        Sensitivity = TP / (TP + FN)\n",
    "        Specificity = TN / (TN + FP)\n",
    "        F1 = 2*(PPV*Sensitivity)/(PPV+Sensitivity)\n",
    "        NPV = TN / (TN + FN)\n",
    "\n",
    "        if epoch > 20:    #25\n",
    "            summary = [train_loss.result().numpy(),train_accuracy.result().numpy(), test_loss.result().numpy(),\n",
    "                          Acc.numpy(), F1, Sensitivity, Specificity, PPV, NPV]\n",
    "            Epoch_summary.append(summary)\n",
    "                \n",
    "\n",
    "        print('Test time: ', time.perf_counter() - t2)\n",
    "        template1 = 'Fold {}, Epoch {}'\n",
    "        template2 = 'Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print(template1.format(fold, epoch))\n",
    "        print(template2.format(loss,\n",
    "                            train_accuracy.result(),\n",
    "                            test_loss.result(),\n",
    "                            Acc))\n",
    "\n",
    "        print(CM)\n",
    "        print('Acc:', float(Acc),'F1:',float(F1), 'Sensitivity:',float(Sensitivity), 'Specificity:', float(Specificity),\n",
    "              'PPV:',float(PPV), 'NPV:',float(NPV))\n",
    "        print('-----------------------------------------------------------------')\n",
    "\n",
    "        \n",
    "    print('Fold time: ', time.perf_counter() - t_fold)\n",
    "    print('Summary for fold: ',fold)\n",
    "    print(Epoch_summary)\n",
    "    epoch_mean = np.mean(Epoch_summary,axis=0)\n",
    "    print('Mean:')\n",
    "    print(epoch_mean)\n",
    "\n",
    "    CV_summary.append(epoch_mean)\n",
    "    \n",
    "print('_________________________________________________________________')     \n",
    "print('Cross validation summary: ')\n",
    "print('Total time: ', time.perf_counter() - t_CV)\n",
    "print(CV_summary)\n",
    "print('Mean:')\n",
    "print(np.mean(CV_summary,axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}